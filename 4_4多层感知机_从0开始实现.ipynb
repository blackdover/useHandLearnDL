{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d27421da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22657f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络结构参数\n",
    "# num_inputs: 输入层神经元数量 (28*28=784，对应Fashion-MNIST图像像素数)\n",
    "# num_outputs: 输出层神经元数量 (10个类别)\n",
    "# num_hiddens: 隐藏层神经元数量 (256个)\n",
    "num_inputs,num_outputs,num_hiddens = 784,10,256\n",
    "\n",
    "# nn.Parameter: 将张量包装为可训练参数，会自动添加到模型的参数列表中\n",
    "# torch.randn: 生成服从标准正态分布N(0,1)的随机张量\n",
    "# requires_grad=True: 启用自动求导，在反向传播时计算梯度\n",
    "# 0.01: 权重初始化的缩放因子，防止梯度爆炸/消失问题\n",
    "\n",
    "# 第一层权重矩阵 W1: (784, 256)\n",
    "W1=nn.Parameter(torch.randn(num_inputs,num_hiddens,requires_grad=True)*0.01)\n",
    "# 第一层偏置向量 b1: (256,)\n",
    "b1=nn.Parameter(torch.zeros(num_hiddens,requires_grad=True))\n",
    "# 第二层权重矩阵 W2: (256, 10)\n",
    "W2=nn.Parameter(torch.randn(num_hiddens,num_outputs,requires_grad=True)*0.01)\n",
    "# 第二层偏置向量 b2: (10,)\n",
    "b2=nn.Parameter(torch.zeros(num_outputs,requires_grad=True))\n",
    "\n",
    "# 将所有参数收集到列表中，便于后续优化器使用\n",
    "params=[W1,b1,W2,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c2f42b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    a=torch.zeros_like(X)\n",
    "    return torch.max(X,a)\n",
    "\n",
    "def net(X):\n",
    "    X=X.reshape((-1,num_inputs))\n",
    "    H=relu(X@W1+b1)\n",
    "    return (H@W2+b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d54a568",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'd2l.torch' has no attribute 'train_ch3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m num_epochs,lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m      5\u001b[0m trainer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(params,lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m----> 6\u001b[0m \u001b[43md2l\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_ch3\u001b[49m(net,train_iter,test_iter,loss,num_epochs,trainer)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'd2l.torch' has no attribute 'train_ch3'"
     ]
    }
   ],
   "source": [
    "loss=nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "num_epochs,lr=10,0.1\n",
    "\n",
    "trainer=torch.optim.SGD(params,lr=lr)\n",
    "d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learntorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
