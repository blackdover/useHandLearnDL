{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c18ebda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "组合网络前向传播结果: tensor([[ 0.2678, -0.1506, -0.3099],\n",
      "        [ 0.7303, -0.7609,  0.2234]], grad_fn=<SubBackward0>)\n",
      "输出的均值（应该接近0）: tensor(0., grad_fn=<MeanBackward0>)\n",
      "\n",
      "分步执行结果: tensor([[ 0.0189, -0.8486, -0.0878],\n",
      "        [ 0.4596,  0.3599,  0.0981]], grad_fn=<SubBackward0>)\n",
      "分步执行输出的均值: tensor(4.9671e-09, grad_fn=<MeanBackward0>)\n",
      "\n",
      "输入张量的梯度形状: torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# CenteredLayer 自定义层的作用：\n",
    "# 这个层实现了数据中心化操作，即将输入张量的每个元素减去该张量的均值\n",
    "# 前向传播的结果就是：输出 = 输入 - 输入的均值\n",
    "# 这样可以使输出数据的均值为0，常用于数据预处理或正则化\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,X):\n",
    "        return X-X.mean()  # 每个值减去均值，实现中心化\n",
    "\n",
    "# 方法1：将CenteredLayer与Linear层组合使用\n",
    "# 创建一个包含Linear层和CenteredLayer的网络\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(5, 3),  # 输入5维，输出3维的线性层\n",
    "    CenteredLayer()   # 对线性层的输出进行中心化\n",
    ")\n",
    "\n",
    "# 测试组合网络\n",
    "input_tensor = torch.randn(2, 5, requires_grad=True)  # 批量大小为2，特征维度为5\n",
    "output = net(input_tensor)\n",
    "print(\"组合网络前向传播结果:\", output)\n",
    "print(\"输出的均值（应该接近0）:\", output.mean())\n",
    "\n",
    "# 方法2：直接对Linear层的输出使用CenteredLayer\n",
    "linear_layer = nn.Linear(5, 3)\n",
    "centered_layer = CenteredLayer()\n",
    "\n",
    "# 先通过线性层，再通过中心化层\n",
    "linear_output = linear_layer(input_tensor)\n",
    "final_output = centered_layer(linear_output)\n",
    "print(\"\\n分步执行结果:\", final_output)\n",
    "print(\"分步执行输出的均值:\", final_output.mean())\n",
    "\n",
    "# 反向传播示例\n",
    "loss = final_output.sum()\n",
    "loss.backward()\n",
    "print(\"\\n输入张量的梯度形状:\", input_tensor.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "345b1a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.5879e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=nn.Sequential(nn.Linear(8,128),CenteredLayer())\n",
    "Y=net(torch.rand(4,8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b7b5b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0597, -0.7242, -0.0976],\n",
       "        [-0.8326, -1.6497, -1.8903],\n",
       "        [ 1.1323, -1.3238,  0.4103],\n",
       "        [ 0.9623, -1.3757, -0.0989],\n",
       "        [ 0.7987,  0.1529, -0.3622]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MyLinear 自定义层的作用：\n",
    "# 这个层实现了一个线性变换 + ReLU激活函数的组合\n",
    "# 相当于 nn.Linear + nn.ReLU 的功能\n",
    "# 前向传播过程：输入 -> 线性变换(矩阵乘法 + 偏置) -> ReLU激活\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self,in_units,units):\n",
    "        super().__init__()\n",
    "        self.weight=nn.Parameter(torch.randn(in_units,units))  # 权重参数\n",
    "        self.bias=nn.Parameter(torch.randn(units,))            # 偏置参数\n",
    "    \n",
    "    def forward(self,X):\n",
    "        # 步骤1：线性变换 (相当于nn.Linear的功能)\n",
    "        linear=torch.matmul(X,self.weight.data)+self.bias.data\n",
    "        # 步骤2：ReLU激活函数 (相当于nn.ReLU的功能)\n",
    "        return F.relu(linear)\n",
    "\n",
    "# 创建自定义层实例\n",
    "linear=MyLinear(5,3)  # 输入5维，输出3维\n",
    "linear.weight         # 查看权重参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "541005e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([2, 5])\n",
      "权重形状: torch.Size([5, 3])\n",
      "输出形状: torch.Size([2, 3])\n",
      "输出结果:\n",
      "tensor([[0.3846, 0.0000, 1.0422],\n",
      "        [0.6881, 0.0000, 0.6264]])\n"
     ]
    }
   ],
   "source": [
    "# 测试自定义层的前向传播\n",
    "input_data = torch.rand(2,5)  # 输入：2个样本，每个样本5个特征\n",
    "output = linear(input_data)   # 输出：2个样本，每个样本3个特征\n",
    "\n",
    "print(f\"输入形状: {input_data.shape}\")  # torch.Size([2, 5])\n",
    "print(f\"权重形状: {linear.weight.shape}\")  # torch.Size([5, 3])\n",
    "print(f\"输出形状: {output.shape}\")  # torch.Size([2, 3])\n",
    "print(f\"输出结果:\\n{output}\")\n",
    "\n",
    "# 矩阵运算解释：\n",
    "# 输入 (2×5) × 权重 (5×3) = 输出 (2×3)\n",
    "# 这是标准的矩阵乘法规则：(m×n) × (n×p) = (m×p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46bad4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7160],\n",
       "        [6.4937]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=nn.Sequential(MyLinear(64,8),MyLinear(8,1))\n",
    "net(torch.rand(2,64))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learntorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
